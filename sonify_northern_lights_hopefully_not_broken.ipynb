{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pretty_midi as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import itertools\n",
    "from IPython.display import Audio\n",
    "import time\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sonify:\n",
    "  \n",
    "  def read_data(self, path):\n",
    "    \"\"\"Read the data from the given csv file path.\"\"\"\n",
    "  \n",
    "    try:\n",
    "      if not path.endswith(\".csv\"): # If provided path is not a valid csv file\n",
    "        print(\"Invalid file path. Must be .csv file.\")\n",
    "        raise FileNotFoundError()\n",
    "      else:\n",
    "        self.df = pd.read_csv(path) # Loads csv file\n",
    "        print(\"Dataset loaded correctly:\", path)\n",
    "\n",
    "      # Constants used to add cumulative seconds values in function inside loop\n",
    "      # entries_per_day = 12*24 # 12 samples an hour (every 5 mins), 24 hours in a day\n",
    "      # seconds_per_day = 60*60*24\n",
    "\n",
    "      # Constants used to create Kp comparison value\n",
    "      self.density_max = np.max(self.df[\"proton_density\"])\n",
    "      self.speed_max = np.max(self.df[\"speed\"])\n",
    "      self.dataset_len = len(self.df.index)\n",
    "\n",
    "      # for i, row in self.df.iterrows(): # Iterates through rows, replacing error data with the average of the previous value and the next valid (i.e., non-error) value\n",
    "        # Data cleanup\n",
    "        # self.__cleanup_column(\"proton_density\", row, i, threshold=0)\n",
    "        # self.__cleanup_column(\"speed\", row, i, threshold=0)\n",
    "        # self.__cleanup_column(\"ion_temp\", row, i, threshold=0)\n",
    "        # self.__cleanup_column(\"bz\", row, i, threshold=-10)\n",
    "        # self.__cleanup_column(\"phi_angle\", row, i, threshold=0)\n",
    "\n",
    "        # Adding column for seconds since start of dataset\n",
    "        # self.__cumulative_seconds(i, entries_per_day, seconds_per_day)\n",
    "\n",
    "        # Adds a column for our own synthetic variable and a column for the difference between that variable and the Kp index.\n",
    "        # self.__kp_comparison(i, row) # Not working fully yet\n",
    "\n",
    "    except Exception as e:\n",
    "      if e.__class__ == FileNotFoundError:\n",
    "        print(\"No valid .csv file found at specified path. Please verify filepath.\")\n",
    "      else:\n",
    "        print(\"Error:\", e.__class__)\n",
    "\n",
    "  \n",
    "  def __cumulative_seconds(self, i, entries_per_day, seconds_per_day):\n",
    "    \"\"\"Adds a value for the amount of seconds passed since the start of the dataset for the current index.\"\"\"\n",
    "    current_day_in_dataset = math.floor(i / entries_per_day) # Gets the number of seconds to the start of the current day by rounding down\n",
    "    self.df.at[i, \"cumulative_secs\"] = (current_day_in_dataset*seconds_per_day) + self.df.at[i, \"sec_of_day\"] # Adds the start of the current day and the seconds elapsed in current day.\n",
    "\n",
    "  def __kp_comparison(self, i, row):\n",
    "    \"\"\"Generates a synthetic variable based on raw data which is compared to the kp index, the difference can\n",
    "    be a variable we can map to something (maybe a filter parameter?)\"\"\"\n",
    "    density_scaled = self.df.at[i, \"proton_density\"]/self.density_max # The density of the current entry scaled 0-1\n",
    "    speed_scaled = self.df.at[i, \"speed\"]/self.speed_max # The speed of the current entry scaled 0-1\n",
    "    phi_angle = self.df.at[i, \"phi_angle\"]\n",
    "\n",
    "    if i == 0: # Edge case for the first index\n",
    "      local_start_index = 0\n",
    "      local_end_index = 2\n",
    "    elif i == self.dataset_len-1: # Edge case for the last index\n",
    "      local_start_index = self.dataset_len - 3\n",
    "      local_end_index = self.dataset_len - 1\n",
    "    else: # All other indices\n",
    "      local_start_index = i - 1\n",
    "      local_end_index = i + 1\n",
    "\n",
    "    local_phi_values = pd.Series.to_numpy(self.df.loc[local_start_index:local_end_index, \"phi_angle\"])\n",
    "    local_time_values = pd.Series.to_numpy(self.df.loc[local_start_index:local_end_index, \"cumulative_secs\"])\n",
    "\n",
    "    kp_equiv = density_scaled + speed_scaled * 9\n",
    "    self.df.at[i, \"kp_equiv\"] = kp_equiv\n",
    "    self.df.at[i, \"kp_diff\"] = row[\"kp_index\"] - kp_equiv\n",
    "\n",
    "  def __cleanup_column(self, column_title, row, i, threshold=0):\n",
    "    \"\"\"Cleans up the data by finding erroneous data, then setting it to the average of the previous cell and the next non-error cell in the specified column.\"\"\"\n",
    "    if row[column_title] < threshold: # If data is below threshold i.e., is an error\n",
    "      next_valid_value = self.__find_next_non_error_cell(i, column_title, threshold) # Find the next non-error cell in the column\n",
    "      self.df.at[i, column_title] = round((self.df.at[i-1, column_title] + next_valid_value)*0.5, 1) # Sets the current cell to the average of the previous cell and the next non-error one\n",
    "\n",
    "  def __find_next_non_error_cell(self, i, column_title, threshold):\n",
    "    \"\"\"Finds the next value in a column above a given threshold using recursion.\"\"\"\n",
    "    next_value = self.df.at[i+1, column_title]\n",
    "    if next_value > threshold: # If the next cell is above the threshold i.e., not an error, return the next cell\n",
    "      return next_value\n",
    "    else: # If the next cell is also an error, run the function again to try the next cell down\n",
    "      return self.__find_next_non_error_cell(i+1, column_title, threshold)\n",
    "\n",
    "  def read_midi(self, path):\n",
    "    \"\"\"Loads the MIDI file, computes grain start points as 1/32 notes.\"\"\"\n",
    "    try:\n",
    "      if not path.endswith(\".mid\"): # Checks for if the path provided is a MIDI file\n",
    "        print(\"Invalid file path. Must be .mid file.\")\n",
    "        raise FileNotFoundError()\n",
    "      else:\n",
    "        midi_data = pm.PrettyMIDI(path) # Loads the midi file\n",
    "        print('MIDI file loaded successfully:', path)\n",
    "\n",
    "      segments = (np.array(pm.PrettyMIDI.get_beats(midi_data,start_time=0.0))) # Gets the beat locations in MIDI file in seconds\n",
    "      self.midi_grain_starts = np.linspace(0, segments[-1], len(segments)*8) # Interpolates down to 1/32 note locations\n",
    "\n",
    "      # Creates attribute for MIDI grain length as the distance between steps in self.midi_grain_starts, converted to samples\n",
    "      self.midi_grain_len = int(self.midi_grain_starts[1] * self.sr)\n",
    "\n",
    "    except Exception as e:\n",
    "      if e.__class__ == FileNotFoundError:\n",
    "        print(\"MIDI file not found. Please verify filepath.\")\n",
    "      else:\n",
    "        print(\"Error:\", e.__class__)\n",
    "\n",
    "  def read_audio(self, path):\n",
    "    self.sr = 48000 # Initiates the sample rate for the sonification process\n",
    "\n",
    "    # Tries to load the audio file, raises FileNotFoundError if not possible.\n",
    "    try:\n",
    "      if not path.endswith(\".wav\"):\n",
    "        raise FileNotFoundError()\n",
    "      else:\n",
    "        self.song = librosa.load(path, sr=self.sr)[0]\n",
    "        print(\"Audio loaded correctly:\", path)\n",
    "\n",
    "    except Exception as e:\n",
    "      if e.__class__ == FileNotFoundError:\n",
    "        print(\"Not a valid .wav file. Please verify filepath.\")\n",
    "      else:\n",
    "        print(\"Error:\", e.__class__)\n",
    "\n",
    "  def __dp_grains_dataframe(self):\n",
    "    \"\"\"Creates the dataframe containing all datapoint grain IDs, start points and feature extraction values.\"\"\"\n",
    "    self.dp_grains_data = pd.DataFrame() # Data frame for grains segmented based on number of data points\n",
    "\n",
    "    song_len_samp = self.song.size # Length of current song in samples\n",
    "    self.total_grains_in_song = self.dataset_len # Number of grains in song (just the length of the dataset)\n",
    "    self.dp_grain_len = math.floor(song_len_samp / self.total_grains_in_song) # The length of each grain in samples\n",
    "\n",
    "    for i in range(self.dataset_len): # Creates one grain for each row of dataframe\n",
    "\n",
    "      self.dp_grains_data.at[i, \"grain_id\"] = i # Adds grain ID to dataframe\n",
    "\n",
    "      grain_start = i*self.dp_grain_len # Find starts sample for datapoint segmented grain\n",
    "      self.dp_grains_data.at[i, \"grain_start\"] = grain_start # Adding grain start sample index to dataframe\n",
    "      \n",
    "      # Feature extraction, returns list of feature values\n",
    "      feature_values = self.__extract_features_from_grain(grain_start, self.dp_grain_len)\n",
    "\n",
    "      # Add features to dataframe\n",
    "      self.dp_grains_data.at[i, \"spec_band\"] = feature_values[0]\n",
    "      self.dp_grains_data.at[i, \"rms\"] = feature_values[1]\n",
    "      self.dp_grains_data.at[i, \"mfcc\"] = feature_values[2]\n",
    "\n",
    "  def __midi_grains_dataframe(self):\n",
    "    \"\"\"Creates the dataframe containing all MIDI grain IDs, start points and feature extraction values.\"\"\"\n",
    "    self.midi_grains_data = pd.DataFrame() # Data frame for grains segmented based on previously segmented MIDI file 1/32 notes\n",
    "\n",
    "    for i in range(self.dataset_len):\n",
    "      self.midi_grains_data.at[i, \"grain_id\"] = i # Adds grain ID to dataframe\n",
    "      \n",
    "      grain_start = int(self.midi_grain_starts[i] * self.sr) # Find starts sample for MIDI segmented grain\n",
    "      self.midi_grains_data.at[i, \"grain_start\"] = grain_start # Adds grain start sample index to dataframe\n",
    "\n",
    "      # Feature extraction, returns list of feature values\n",
    "      feature_values = self.__extract_features_from_grain(grain_start, self.midi_grain_len)\n",
    "\n",
    "      # Add features to dataframe\n",
    "      self.midi_grains_data.at[i, \"spec_band\"] = feature_values[0]\n",
    "      self.midi_grains_data.at[i, \"rms\"] = feature_values[1]\n",
    "      self.midi_grains_data.at[i, \"mfcc\"] = feature_values[2]\n",
    "\n",
    "  def build_grains_dataframes(self):\n",
    "    \"\"\"Utility method to create dataframes for both types of segmentation, by datapoints and by MIDI\"\"\"\n",
    "    self.__dp_grains_dataframe()\n",
    "    self.__midi_grains_dataframe()\n",
    "\n",
    "  def __extract_features_from_grain(self, grain_start, grain_len):\n",
    "    \"\"\"Receives grain position info and computes spectral bandwidth, RMS and MFCC Librosa feature extraction. Returns results.\"\"\"\n",
    "    grain_start = int(grain_start)\n",
    "    grain_len = int(grain_len)\n",
    "    \n",
    "    grain = self.song[grain_start:grain_start+grain_len] # Selecting grain data based on start and end points\n",
    "\n",
    "    # Spectral Bandwidth\n",
    "    spec_band = librosa.feature.spectral_bandwidth(y=grain, sr=self.sr, n_fft=grain_len, hop_length=grain_len, win_length=grain_len)\n",
    "    \n",
    "    # Root Mean Square\n",
    "    rms = librosa.feature.rms(y=grain, frame_length=grain_len, hop_length=grain_len)\n",
    "    \n",
    "    # Mel Frequency Cepstrum Coefficients\n",
    "    mfcc_coeffs = librosa.feature.mfcc(y=grain, sr=self.sr, n_mels=13, n_fft=grain_len, hop_length=grain_len, win_length=grain_len)\n",
    "    mfcc = math.sqrt(np.max(mfcc_coeffs)) # Gets sqrt of largest coefficient, to make range more usable.\n",
    "\n",
    "    return [spec_band, rms, mfcc]\n",
    "\n",
    "  def __map_feature_iterator(self, feature_list, datapoints_list, grains_dataframe, grains_type=\"dp\"):    \n",
    "    \"\"\"\"Maps the provided list of features to the provided list of datapoints in the dataset via assigning grain IDs.\n",
    "        Called inside the map_features() method.\"\"\"\n",
    "    \n",
    "    try:\n",
    "      if grains_type not in [\"dp\", \"midi\"]:\n",
    "        raise ValueError()\n",
    "\n",
    "      mapping_start_time = time.time()\n",
    "\n",
    "      for i, feature in enumerate(feature_list):\n",
    "\n",
    "        # MIDI grains data sorted by current feature\n",
    "        feature_sorted = grains_dataframe[[\"grain_id\",feature]].sort_values(by=feature, axis='index', kind=\"mergesort\", ignore_index=True)\n",
    "        # Dataset dataset by current mapped value in datapoints_list\n",
    "        self.df.sort_values(by=datapoints_list[i], axis=\"index\", kind=\"mergesort\", ignore_index=True, inplace=True)\n",
    "\n",
    "        grain_id_column_title = grains_type + \"_\" + feature + \"_grain_id\" # Assembling new grain id column title for self.df\n",
    "        feature_sorted.columns = [grain_id_column_title, feature] # Renaming columns in feature_sorted based on new titles\n",
    "\n",
    "        # Adds the new grain ID column to the dataset self.df\n",
    "        self.df = pd.concat([self.df, feature_sorted[grain_id_column_title]], axis=\"columns\")\n",
    "\n",
    "        # Re-sorts sorted self.df dataset\n",
    "        self.df.sort_values(by=\"cumulative_secs\", axis=\"index\", kind=\"mergesort\", ignore_index=True, inplace=True)\n",
    "\n",
    "      # Determines time to map features for the current grain type and prints the result\n",
    "      mapping_exec_time = time.time() - mapping_start_time\n",
    "      print(\"Mapping \", grains_type, \"grains completed in\", round(mapping_exec_time, 3), \"secs\")\n",
    "\n",
    "    except Exception as e:\n",
    "      if e.__class__ == ValueError:\n",
    "        print(\"Ensure grains_type is either 'dp' or 'midi'.\")\n",
    "      else:\n",
    "        print(\"Error:\", e.__class__)\n",
    "  \n",
    "  def map_features(self):\n",
    "    \"\"\"Utility method which adds grain ID columns for both grain types to dataframe by calling __map_feature_iterator method\"\"\"\n",
    "    feature_list = [\"spec_band\", \"rms\", \"mfcc\"] # Features to be mapped\n",
    "    datapoints_list = [\"proton_density\", \"bz\", \"ion_temp\"] # Datapoints to be mapped\n",
    "    \n",
    "    # For datapoint grains\n",
    "    grains_type = \"dp\"\n",
    "    self.__map_feature_iterator(feature_list, datapoints_list, self.dp_grains_data, grains_type)\n",
    "\n",
    "    # For MIDI grains\n",
    "    grains_type = \"midi\"\n",
    "    self.__map_feature_iterator(feature_list, datapoints_list, self.midi_grains_data, grains_type)\n",
    "\n",
    "  def __sum_grains(self, list_of_grains):\n",
    "    \"\"\"Adds together the individual grains for each datapoint into the full grain.\"\"\"\n",
    "    # how do we want to compound the grains?\n",
    "    # just sums them for now\n",
    "    return np.sum(list_of_grains, axis=0)\n",
    "    \n",
    "  def scale_features(self):\n",
    "    \"\"\"Scale the bz and speed data fields from self.df dataset to required ranges as audio processing parameters.\"\"\"\n",
    "    \n",
    "    # Scales Bz to between 1 and -1\n",
    "    bz = self.df['bz'].copy()\n",
    "    bz = bz/np.max(bz)\n",
    "    self.df['bz_scaled'] = bz\n",
    "    \n",
    "    # Scales speed to between 1 and 2\n",
    "    speed = self.df['speed'].copy()\n",
    "    speed -= speed.min()\n",
    "    speed /= speed.max()\n",
    "    speed += 1\n",
    "    self.df['speed_scaled'] = speed\n",
    "    \n",
    "    print('Scaling of bz and speed values complete and written to dataframe.')\n",
    "    \n",
    "  def __apply_bz(self, grain, i):\n",
    "    \"\"\"If bz is positive, apply low-pass filter. If bz is negative, apply high-pass filter.\n",
    "        Uses abs(bz) to create scaled cutoff value for use in filter.\"\"\"\n",
    "    bz_scaled = self.df.iloc[i]['bz_scaled']\n",
    "    \n",
    "    normal_cutoff = (abs(bz_scaled)*0.5) + 0.01 # Generates the cutoff value for signal.butter, in range 0.01-0.5\n",
    "    order = 5\n",
    "    b, a = scipy.signal.butter(order, normal_cutoff, btype=('low' if bz_scaled > 0 else 'high'),\n",
    "                               analog=False)\n",
    "    return scipy.signal.filtfilt(b, a, grain)\n",
    "    \n",
    "  def __apply_speed(self, grain, i, grain_len):\n",
    "    \"\"\"Change playback speed based on scaled speed value in dataset.\"\"\"\n",
    "    speed = self.df.iloc[i]['speed_scaled'] # Gets the scaled speed value.\n",
    "    grain = librosa.effects.time_stretch(grain, rate=speed) # Applies the time-stretching to the grain.\n",
    "    grain = np.append(grain, grain) # Duplicates the time-stretched grain, so it still fills the grain length if it's sped up.\n",
    "    grain = grain[:grain_len] # Trimming the duplicated grain to the grain length.\n",
    "    \n",
    "    return grain\n",
    "\n",
    "  def __apply_phi(self, grain, i):\n",
    "    \"\"\"Change playback direction based on sign of phi angle value in dataset.\"\"\"\n",
    "    phi = self.df.iloc[i]['phi_angle']\n",
    "    \n",
    "    return np.flip(grain) if phi < 0 else grain\n",
    "\n",
    "  def __simple_envelope(self, grain, grain_len):\n",
    "    \"\"\"Apply a simple envelope (100-sample-long fade on each end) to the grain.\"\"\"\n",
    "    a = np.arange(0,1,0.01) # Ramp up of the envelope\n",
    "    len_a = len(a) # The length of the ramp\n",
    "    flat = np.ones((grain_len-2 * len_a)) # Flat middle section of envelope (all 1 values)\n",
    "    rev_a = np.flip(a) # Creating ramp down of envelope by flipping the ramp up\n",
    "    env = np.append(a,flat) # Adding ramp up and flat to envelope\n",
    "    env = np.append(env,rev_a) # Adding together all the pieces of the envelope\n",
    "\n",
    "    return grain * env\n",
    "    \n",
    "  def __compound_grains(self, grains_type='dp'):\n",
    "    \"\"\"Finds, sums, processes and appends all relevant grains from the dataset.\n",
    "      Uses grains_type string to determine whether to use datapoint or MIDI segmentation.\"\"\"\n",
    "    \n",
    "    synth_start_time = time.time() # Used for measuring execution time of the loop.\n",
    "    \n",
    "    # Tries to assign the dataframe, columns and grain length based on grains_type. Generates error if invalid input.\n",
    "    try:\n",
    "      if grains_type == 'dp':\n",
    "        cols = ['dp_spec_band_grain_id', 'dp_rms_grain_id', 'dp_mfcc_grain_id']\n",
    "        grains_dataframe = self.dp_grains_data\n",
    "        grain_len = self.dp_grain_len\n",
    "      elif grains_type == 'midi':\n",
    "        cols = ['midi_spec_band_grain_id', 'midi_rms_grain_id', 'midi_mfcc_grain_id']\n",
    "        grains_dataframe = self.midi_grains_data\n",
    "        grain_len = self.midi_grain_len\n",
    "      else:\n",
    "        raise ValueError()\n",
    "\n",
    "    except ValueError:\n",
    "      print(\"grains_type must be either 'dp' or 'midi'.\")\n",
    "\n",
    "    # Main loop build the output from individual summed then processed grains\n",
    "    grains_attached = [] # Initialises main return list containing all audio\n",
    "    for i, row in self.df.iterrows(): # Iterating through rows of the dataset\n",
    "      list_grains = [] # Resets list containing the data from the song to be summed to make compound grain\n",
    "      for c in cols: # Loops through columns to pull grain audio samples\n",
    "        grain_start = grains_dataframe[grains_dataframe['grain_id'] == row[c]]['grain_start'].values[0] # Gets grain based on grain ID in current column\n",
    "        grain_end = grain_start + grain_len # Calculates grain end point\n",
    "        grain = self.song[int(grain_start):int(grain_end)] # Pulls the grain audio\n",
    "        list_grains.append(grain) # Appends to list of grains for current row in data\n",
    "      compound_grain = self.__sum_grains(list_grains) # Sums the selected grains using \n",
    "        \n",
    "      # print(i, '/', self.total_grains_in_song, ' merging grains...', end='\\r')\n",
    "      \n",
    "      # On each compound grain, apply processing from the mappings\n",
    "      compound_grain = self.__apply_bz(compound_grain, i) # Applies lp/hp filter\n",
    "      compound_grain = self.__apply_speed(compound_grain, i, grain_len) # Changes playback speed\n",
    "      compound_grain = self.__apply_phi(compound_grain, i) # Changes playback direction\n",
    "      \n",
    "      # Simple envelope to ramp start and end\n",
    "      compound_grain = self.__simple_envelope(compound_grain, grain_len)\n",
    "      \n",
    "      # Append to main output audio array\n",
    "      grains_attached = np.append(grains_attached, compound_grain)\n",
    "    \n",
    "    synth_exec_time = time.time() - synth_start_time\n",
    "    print(\"Completed assembly of\", grains_type, \"grains in\", round(synth_exec_time, 3), \"secs\")\n",
    "\n",
    "    return grains_attached\n",
    "\n",
    "  def synthesise_both(self):\n",
    "    \"\"\"Utility method to assemble both types of segmentation\"\"\"\n",
    "    self.dp_synth = self.__normalise(self.__compound_grains(\"dp\"))\n",
    "    self.midi_synth = self.__normalise(self.__compound_grains(\"midi\"))\n",
    "\n",
    "    return (self.dp_synth, self.midi_synth)\n",
    "\n",
    "  def __normalise(self, audio):\n",
    "    \"\"\"Utility method to normalise the given audio.\"\"\"\n",
    "    audio_max = np.max(np.abs(audio))\n",
    "    norm_audio = audio*1/audio_max\n",
    "    return norm_audio\n",
    "\n",
    "  def __apply_reverb(self, path, grains_type=\"dp\"):\n",
    "    \"\"\"Applies convolution reverb audios in sections of 36 grains, as Kp-index only updates every 3 hours/36 rows of dataset.\"\"\"\n",
    "    rev_start_time = time.time()\n",
    "    \n",
    "    # Allows method to determine which audio to use based on grains_type string, raises error if not valid option\n",
    "    try:\n",
    "      if grains_type == \"dp\":\n",
    "        synth = self.dp_synth\n",
    "        grain_len = self.dp_grain_len\n",
    "      elif grains_type == \"midi\":\n",
    "        synth = self.midi_synth\n",
    "        grain_len = self.midi_grain_len\n",
    "      else:\n",
    "        raise ValueError()\n",
    "    except ValueError as e:\n",
    "      print(\"grains_type must be either 'dp' or 'midi'.\")\n",
    "      \n",
    "    output_reverb = np.zeros(synth.size)\n",
    "    kp_section_no_of_datapoints = 36 # Number of datapoints in a Kp section\n",
    "    kp_section_len = kp_section_no_of_datapoints*grain_len # Length of a Kp section in samples\n",
    "\n",
    "    # Tries to load the impulse response and makes sure it is shorter than one section.\n",
    "    try:\n",
    "      ir = librosa.load(path, sr=self.sr, mono=True)[0] # Loads the IR\n",
    "      ir = self.__normalise(ir) # Normalises the IR\n",
    "      if ir.size > kp_section_len: # The IR can't be longer than the kp_section, so this checks.\n",
    "        raise ValueError()\n",
    "\n",
    "    except Exception as e:\n",
    "      if e.__class__ == ValueError: # If the IR is too long, just return the un-reverbed audio\n",
    "        print(\"You must use an impulse response that is shorter than\", kp_section_len/self.sr, \".\")\n",
    "        print(\"Returning non-reverbed audio...\")\n",
    "        return synth\n",
    "      elif e.__class__ == FileNotFoundError: # If the IR isn't a valid .wav, just return the un-reverbed audio.\n",
    "        print(\"Your impulse response must be a valid .wav file.\")\n",
    "        print(\"Returning non-reverbed audio...\")\n",
    "        return synth\n",
    "      else:\n",
    "        print(\"Error:\", e.__class__)\n",
    "        print(\"Returning non-reverbed audio...\")\n",
    "        return synth\n",
    "\n",
    "    ir_padded = np.pad(ir, (0, kp_section_len - ir.size)) # Padding the IR to the same size as the section\n",
    "\n",
    "    for i in range(0, self.dataset_len, kp_section_no_of_datapoints): # Loop for number of sections\n",
    "      kp_section_start = i*grain_len # Start point in samples of current section\n",
    "      kp_section = synth[kp_section_start:kp_section_start + kp_section_len] # Gets the actual section data\n",
    "\n",
    "      if kp_section.size < ir_padded.size: # Edge case for final section, as padded IR array may be longer than remaining data\n",
    "        np.pad(kp_section, (0, ir_padded.size - kp_section.size)) # Pads final section to length of padded IR\n",
    "\n",
    "      ir_scale_factor = self.df.at[i, \"kp_index\"] / 9\n",
    "      dry_scale_factor = 1 - ir_scale_factor # Creates a scale factor for dry signal, which is the inverse of ir_scale_factor\n",
    "      ir_padded_scaled = ir_padded * ir_scale_factor # Scaling the IR based on Kp index\n",
    "\n",
    "      Kp_Section = scipy.fft.fft(kp_section) # FFT of the kp_section\n",
    "      Ir_Padded_Scaled = scipy.fft.fft(ir_padded_scaled) # FFT of the padded scaled IR\n",
    "\n",
    "      Kp_Section_Reverb = Kp_Section * Ir_Padded_Scaled # Frequency domain multiplication\n",
    "\n",
    "      kp_section_reverb = np.real(scipy.fft.ifft(Kp_Section_Reverb)) # Returns the real section in the time domain\n",
    "\n",
    "      kp_section_mix = kp_section_reverb + (dry_scale_factor*kp_section) # Adding some of the dry signal back in for makeup gain\n",
    "\n",
    "      for sample_index, sample in enumerate(kp_section_mix): # Loops through mixed section\n",
    "        output_index = i * grain_len + sample_index # Corresponding index in output is the previous number of sections plus the current sample number\n",
    "        output_reverb[output_index] = sample # Adding mixed sample to output\n",
    "\n",
    "    output_reverb = self.__normalise(output_reverb)\n",
    "\n",
    "    rev_exec_time = time.time() - rev_start_time\n",
    "    print(\"Applied reverb to\", grains_type, \"grains in\", round(rev_exec_time, 3), \"secs\")\n",
    "\n",
    "    output_filepath = \"./output/\" + grains_type + \"_grains_output.wav\"\n",
    "    print(\"Writing output to:\", output_filepath)\n",
    "    sf.write(output_filepath, output_reverb, self.sr)\n",
    "\n",
    "    return output_reverb\n",
    "\n",
    "  def reverb_both(self, path):\n",
    "    \"\"\"Utility method to apply reverb to both audios.\"\"\"\n",
    "    dp_synth_reverb = self.__apply_reverb(path, \"dp\")\n",
    "    midi_synth_reverb = self.__apply_reverb(path, \"midi\")\n",
    "\n",
    "    return (dp_synth_reverb, midi_synth_reverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sonify = Sonify()\n",
    "sonify.read_data(\"./output_jack.csv\") # solar_wind_data_2003-10-27 - 2003-11-02_ACTUAL.csv\")\n",
    "sonify.read_audio(\"./corpus/02_Dido White Flag.wav\")\n",
    "sonify.read_midi(\"./corpus/02_Dido White Flag_adjusted_2.mid\")\n",
    "sonify.build_grains_dataframes()\n",
    "sonify.map_features()\n",
    "\n",
    "# do necessary scaling inside a function\n",
    "sonify.scale_features()\n",
    "\n",
    "# compound grains and apply mapping\n",
    "synth_outputs = sonify.synthesise_both()\n",
    "\n",
    "display(Audio(synth_outputs[0], rate=sonify.sr))\n",
    "plt.plot(synth_outputs[0])\n",
    "plt.show()\n",
    "\n",
    "display(Audio(synth_outputs[1], rate=sonify.sr))\n",
    "plt.plot(synth_outputs[1])\n",
    "plt.show()\n",
    "\n",
    "# apply the reverb\n",
    "outputs_reverb = sonify.reverb_both(\"./ir/ir_sydney_cathedral.wav\")\n",
    "\n",
    "display(Audio(outputs_reverb[0], rate=sonify.sr))\n",
    "display(Audio(outputs_reverb[1], rate=sonify.sr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with the first 100 grains, workes mmm :) -emin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to test different sizes, I trim it in the big class cell above, line 302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hear\n",
    "print(attached.shape)\n",
    "plt.plot(attached)\n",
    "Audio(data=attached, rate=48000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just for fun plots - selected grains based on different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpp\n",
    "plt.figure().set_size_inches(16,4)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('grain ids')\n",
    "plt.scatter(x=sonify.df['cumulative_secs'], y=sonify.df[['dp_spec_band_grain_id']], c='r', s=5)\n",
    "plt.scatter(x=sonify.df['cumulative_secs'], y=sonify.df[['dp_rms_grain_id']], c='g', s=5)\n",
    "plt.scatter(x=sonify.df['cumulative_secs'], y=sonify.df[['dp_mfcc_grain_id']], c='b', s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi version\n",
    "plt.figure().set_size_inches(16,4)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('grain ids')\n",
    "plt.scatter(x=sonify.df['cumulative_secs'], y=sonify.df[['midi_spec_band_grain_id']], c='r', s=5)\n",
    "plt.scatter(x=sonify.df['cumulative_secs'], y=sonify.df[['midi_rms_grain_id']], c='g', s=5)\n",
    "plt.scatter(x=sonify.df['cumulative_secs'], y=sonify.df[['midi_mfcc_grain_id']], c='b', s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.stft(attached)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "plt.figure()\n",
    "plt.figure().set_size_inches(24,5)\n",
    "librosa.display.specshow(S_db)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying entire data (bad output. also no sound even though we can see the audio there around 0, no idea for now -emin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hear\n",
    "print(attached.shape)\n",
    "plt.plot(attached)\n",
    "Audio(data=attached, rate=48000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulating how apply_speed() method works - may not work, nvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_speed(data):\n",
    "    # change playback speed based on speed\n",
    "    # fill up the gap by looping if needed ??\n",
    "    speed = 1.5 #sonify.df.iloc[0]['speed_scaled']\n",
    "    data = librosa.effects.time_stretch(data, rate=speed)\n",
    "    data = np.append(data, data) # duplicate data, append one to other (cannot be more then twice as max speed value is 2)\n",
    "    data = data[:sonify.grain_len] # trim\n",
    "    return data\n",
    "data, sr = librosa.load(librosa.ex('choice'))\n",
    "data = data[:sonify.grain_len]\n",
    "plt.plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(apply_speed(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "33e85536a753e4289d1e3ca1904fea47091c61e030b0b2f85895695a2354c09d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
