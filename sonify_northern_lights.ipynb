{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pretty_midi as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import math\n",
    "import jdc\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sonify:\n",
    "  \n",
    "  def read_data(self, path):\n",
    "    \"\"\"Read the data from the given csv file path.\"\"\"\n",
    "    if not path.endswith(\".csv\"): # If provided path is not a valid csv file\n",
    "      print(\"Invalid file path. Must be .csv file.\")\n",
    "      raise FileNotFoundError()\n",
    "    self.df = pd.read_csv(path) # Loads csv file\n",
    "\n",
    "    # Constants used to add cumulative seconds values in function inside loop\n",
    "    entries_per_day = 12*24 # 12 samples an hour (every 5 mins), 24 hours in a day\n",
    "    seconds_per_day = 60*60*24\n",
    "\n",
    "    # Constants used to create Kp comparison value\n",
    "    self.density_max = np.max(self.df[\"proton_density\"])\n",
    "    self.speed_max = np.max(self.df[\"speed\"])\n",
    "\n",
    "    for i, row in self.df.iterrows(): # Iterates through rows, replacing error data with the average of the previous value and the next valid (i.e., non-error) value\n",
    "      # Data cleanup\n",
    "      self.__cleanup_column(\"proton_density\", row, i, threshold=0)\n",
    "      self.__cleanup_column(\"speed\", row, i, threshold=0)\n",
    "      self.__cleanup_column(\"ion_temp\", row, i, threshold=0)\n",
    "      self.__cleanup_column(\"bz\", row, i, threshold=-10)\n",
    "      self.__cleanup_column(\"phi_angle\", row, i, threshold=0)\n",
    "\n",
    "      # Adding column for seconds since start of dataset\n",
    "      self.__cumulative_seconds(i, entries_per_day, seconds_per_day)\n",
    "\n",
    "      # Adds a column for our own synthetic variable and a column for the difference between that variable and the Kp index.\n",
    "      self.__kp_comparison(i, row) # Not working fully yet\n",
    "\n",
    "    # Just for testing...\n",
    "    plt.plot(self.df[\"cumulative_secs\"], self.df[\"kp_index\"])\n",
    "    plt.title(\"KP Index over time\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(self.df[\"cumulative_secs\"], self.df[\"kp_equiv\"])\n",
    "    plt.title(\"KP Equiv over time\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(self.df[\"cumulative_secs\"], self.df[\"kp_diff\"])\n",
    "    plt.title(\"KP difference over time\")\n",
    "    plt.show()\n",
    "\n",
    "    # print(\"Lowest density value:\", np.min(self.df[\"proton_density\"]))\n",
    "    # print(\"Lowest speed value:\", np.min(self.df[\"speed\"]))\n",
    "\n",
    "    print(self.df)\n",
    "    # read the data into df\n",
    "    # TODO check if path is ok, file format is ok, load data into dataframe\n",
    "  \n",
    "  def __cumulative_seconds(self, i, entries_per_day, seconds_per_day):\n",
    "    \"\"\"Adds a value for the amount of seconds passed since the start of the dataset for the current index.\"\"\"\n",
    "    current_day_in_dataset = math.floor(i / entries_per_day) # Gets the number of seconds to the start of the current day by rounding down\n",
    "    self.df.at[i, \"cumulative_secs\"] = (current_day_in_dataset*seconds_per_day) + self.df.at[i, \"sec_of_day\"] # Adds the start of the current day and the seconds elapsed in current day.\n",
    "\n",
    "  def __kp_comparison(self, i, row):\n",
    "    \"\"\"Generates a synthetic variable based on raw data which is compared to the kp index, the difference can\n",
    "    be a variable we can map to something (maybe a filter parameter?)\"\"\"\n",
    "    density_scaled = self.df.at[i, \"proton_density\"]/self.density_max # The density of the current entry scaled 0-1\n",
    "    speed_scaled = self.df.at[i, \"speed\"]/self.speed_max # The speed of the current entry scaled 0-1\n",
    "    phi_angle = self.df.at[i, \"phi_angle\"]\n",
    "\n",
    "    if i == 0: # Edge case for the first index\n",
    "      local_start_index = 0\n",
    "      local_end_index = 2\n",
    "    elif i == len(self.df.index)-1: # Edge case for the last index\n",
    "      local_start_index = len(self.df.index) - 3\n",
    "      local_end_index = len(self.df.index) - 1\n",
    "    else: # All other indices\n",
    "      local_start_index = i - 1\n",
    "      local_end_index = i + 1\n",
    "\n",
    "    local_phi_values = pd.Series.to_numpy(self.df.loc[local_start_index:local_end_index, \"phi_angle\"])\n",
    "    local_time_values = pd.Series.to_numpy(self.df.loc[local_start_index:local_end_index, \"cumulative_secs\"])\n",
    "\n",
    "    kp_equiv = density_scaled + speed_scaled * 9\n",
    "    self.df.at[i, \"kp_equiv\"] = kp_equiv\n",
    "    self.df.at[i, \"kp_diff\"] = row[\"kp_index\"] - kp_equiv\n",
    "\n",
    "  def __cleanup_column(self, column_title, row, i, threshold=0):\n",
    "    \"\"\"Cleans up the data by finding erroneous data, then setting it to the average of the previous cell and the next non-error cell in the specified column.\"\"\"\n",
    "    if row[column_title] < threshold: # If data is below threshold i.e., is an error\n",
    "      next_valid_value = self.__find_next_non_error_cell(i, column_title, threshold) # Find the next non-error cell in the column\n",
    "      self.df.at[i, column_title] = round((self.df.at[i-1, column_title] + next_valid_value)*0.5, 1) # Sets the current cell to the average of the previous cell and the next non-error one\n",
    "\n",
    "  def __find_next_non_error_cell(self, i, column_title, threshold):\n",
    "    \"\"\"Finds the next value in a column above a given threshold using recursion.\"\"\"\n",
    "    next_value = self.df.at[i+1, column_title]\n",
    "    if next_value > threshold: # If the next cell is above the threshold i.e., not an error, return the next cell\n",
    "      return next_value\n",
    "    else: # If the next cell is also an error, run the function again to try the next cell down\n",
    "      return self.__find_next_non_error_cell(i+1, column_title, threshold)\n",
    "\n",
    "  def read_midi(self, path):\n",
    "\n",
    "    \"\"\"Read the midifile into a dataframe.\"\"\"\n",
    "    \n",
    "    midi_data = pm.PrettyMIDI(path)\n",
    "    if not path.endswith(\".mid\"):\n",
    "      print(\"Invalid file path. Must be .mid file.\")\n",
    "      raise FileNotFoundError()\n",
    "    else: print('MIDI-Fileload successful.')\n",
    "\n",
    "    segments = (np.array(pm.PrettyMIDI.get_beats(midi_data,start_time=0.0)))\n",
    "    print((segments))\n",
    "    print(segments[-1])\n",
    "    self.midi_grain_start = np.linspace(0, segments[-1], len(segments)*8)\n",
    "    print(len(self.midi_grain_start))\n",
    "\n",
    "    # read midi and audio files, path should contain all files in folder\n",
    "    # TODO check if path is ok, file formats are ok, ...\n",
    "\n",
    "  def granulate(self, corpus):\n",
    "    # Idea for how to reference grains:\n",
    "    # Use another dataframe, then we can store Librosa features etc alongside each grain in a table\n",
    "    # We can reference each grain by start and end indices in the numpy array of the audio\n",
    "\n",
    "    self.grains_data = pd.DataFrame()\n",
    "\n",
    "    for song in corpus:\n",
    "      \n",
    "      song_len_samp = song.size # Length of current song in samples\n",
    "      total_grains_in_song = len(self.df.index) # Number of grains in song (just the length of the dataset)\n",
    "      grain_len_samp = math.floor(song_len_samp / total_grains_in_song) # The length of each grain in samples\n",
    "      \n",
    "      for i, row in self.df.iterrows():\n",
    "        self.grains_data[\"song_no\"] = 0 # We'll need a column to tell which song a grain is coming from\n",
    "        self.grain_data[\"grain_in_song\"] = i # The current grain number within the current song\n",
    "        grain_start_index = i * grain_len_samp # The index for the start sample of the current grain\n",
    "        grain_end_index = (i * grain_len_samp) + grain_len_samp # The index for the end sample of the current grain\n",
    "        self.grain_data[\"grain_pos\"] = [grain_start_index, grain_end_index] # Adding grain location to dataframe as list\n",
    "        # Any Librosa features could go here\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this syntax to add methods to Sonify class in other cells\n",
    "\n",
    "%%add_to Sonify\n",
    "def function(self):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fileload successful.\n",
      "[  0.         0.705882   1.411764   2.117646   2.823528   3.52941\n",
      "   4.235292   4.941174   5.647056   6.352938   7.05882    7.764702\n",
      "   8.470584   9.176466   9.882348  10.58823   11.294112  11.999994\n",
      "  12.705876  13.411758  14.11764   14.823522  15.529404  16.235286\n",
      "  16.941168  17.64705   18.352932  19.058814  19.764696  20.470578\n",
      "  21.17646   21.882342  22.588224  23.294106  23.999988  24.70587\n",
      "  25.411752  26.117634  26.823516  27.529398  28.23528   28.941162\n",
      "  29.647044  30.352926  31.058808  31.76469   32.470572  33.176454\n",
      "  33.882336  34.588218  35.2941    35.999982  36.705864  37.411746\n",
      "  38.117628  38.82351   39.529392  40.235274  40.941156  41.647038\n",
      "  42.35292   43.058802  43.764684  44.470566  45.176448  45.88233\n",
      "  46.588212  47.294094  47.999976  48.705858  49.41174   50.117622\n",
      "  50.823504  51.529386  52.235268  52.94115   53.647032  54.352914\n",
      "  55.058796  55.764678  56.47056   57.176442  57.882324  58.588206\n",
      "  59.294088  59.99997   60.705852  61.411734  62.117616  62.823498\n",
      "  63.52938   64.235262  64.941144  65.647026  66.352908  67.05879\n",
      "  67.764672  68.470554  69.176436  69.882318  70.5882    71.294082\n",
      "  71.999964  72.705846  73.411728  74.11761   74.823492  75.529374\n",
      "  76.235256  76.941138  77.64702   78.352902  79.058784  79.764666\n",
      "  80.470548  81.17643   81.882312  82.588194  83.294076  83.999958\n",
      "  84.70584   85.411722  86.117604  86.823486  87.529368  88.23525\n",
      "  88.941132  89.647014  90.352896  91.058778  91.76466   92.470542\n",
      "  93.176424  93.882306  94.588188  95.29407   95.999952  96.705834\n",
      "  97.411716  98.117598  98.82348   99.529362 100.235244 100.941126\n",
      " 101.647008 102.35289  103.058772 103.764654 104.470536 105.176418\n",
      " 105.8823   106.588182 107.294064 107.999946 108.705828 109.41171\n",
      " 110.117592 110.823474 111.529356 112.235238 112.94112  113.647002\n",
      " 114.352884 115.058766 115.764648 116.47053  117.176412 117.882294\n",
      " 118.588176 119.294058 119.99994  120.705822 121.411704 122.117586\n",
      " 122.823468 123.52935  124.235232 124.941114 125.646996 126.352878\n",
      " 127.05876  127.764642 128.470524 129.176406 129.882288 130.58817\n",
      " 131.294052 131.999934 132.705816 133.411698 134.11758  134.823462\n",
      " 135.529344 136.235226 136.941108 137.64699  138.352872 139.058754\n",
      " 139.764636 140.470518 141.1764   141.882282 142.588164 143.294046\n",
      " 143.999928 144.70581  145.411692 146.117574 146.823456 147.529338\n",
      " 148.23522  148.941102 149.646984 150.352866 151.058748 151.76463\n",
      " 152.470512 153.176394 153.882276 154.588158 155.29404  155.999922\n",
      " 156.705804 157.411686 158.117568 158.82345  159.529332 160.235214\n",
      " 160.941096 161.646978 162.35286  163.058742 163.764624 164.470506\n",
      " 165.176388 165.88227  166.588152 167.294034 167.999916 168.705798\n",
      " 169.41168  170.117562 170.823444 171.529326 172.235208 172.94109\n",
      " 173.646972 174.352854 175.058736 175.764618 176.4705   177.176382\n",
      " 177.882264 178.588146 179.294028 179.99991  180.705792 181.411674\n",
      " 182.117556 182.823438 183.52932  184.235202 184.941084 185.646966\n",
      " 186.352848 187.05873  187.764612 188.470494 189.176376 189.882258\n",
      " 190.58814  191.294022 191.999904 192.705786 193.411668 194.11755\n",
      " 194.823432 195.529314 196.235196 196.941078 197.64696  198.352842\n",
      " 199.058724 199.764606 200.470488 201.17637  201.882252 202.588134\n",
      " 203.294016 203.999898 204.70578  205.411662 206.117544 206.823426\n",
      " 207.529308 208.23519  208.941072 209.646954 210.352836 211.058718\n",
      " 211.7646   212.470482 213.176364 213.882246 214.588128 215.29401\n",
      " 215.999892 216.705774 217.411656 218.117538 218.82342  219.529302\n",
      " 220.235184 220.941066]\n",
      "220.94106600000066\n",
      "2512\n"
     ]
    }
   ],
   "source": [
    "sonify = Sonify()\n",
    "#sonify.read_data(\"/content/drive/MyDrive/Python Assignment 4 Depot/solar_wind_data_2003-10-27 - 2003-11-02_ACTUAL.csv\")\n",
    "sonify.read_midi(\"corpus/02_Dido White Flag_adjusted_2.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('MCT4001')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00e9223df2505016e83dd65347a75f8382f438b2bcd606c5a522cb0e4c79c077"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
